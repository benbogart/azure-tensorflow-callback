{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "posted-rates",
   "metadata": {},
   "source": [
    "# Notebook # 2\n",
    "\n",
    "This is the second notebook for the `log-to-azure` example. In this notebook we load our resources and run the model.\n",
    "\n",
    "For the sake of clarity all packages and modules are imported in the cells in which they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-modification",
   "metadata": {},
   "source": [
    "# Load all assets.\n",
    "Load in the assets we created in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alien-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Environment\n",
    "\n",
    "# Load the stored workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the registered dataset from azure\n",
    "dataset = Dataset.get_by_name(ws, name='waste_images')\n",
    "\n",
    "## Try with our saved image\n",
    "env = Environment.get(workspace=ws, name=\"waste-env-gpu\")\n",
    "\n",
    "# get our compute target\n",
    "compute_target = ws.compute_targets[\"gpu-cluster-NC6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-powder",
   "metadata": {},
   "source": [
    "## Define experiment name\n",
    "An experiment is a container for one or more model runs.  You can group iterations of a model into an experiment for easy access on azureml Studio.  Every run must be in an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consecutive-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# define the expiriment\n",
    "exp = Experiment(workspace=ws, name='recycling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-lesson",
   "metadata": {},
   "source": [
    "## Create train script\n",
    "The train script is what will be executed on the azureml compute instance.  The directory specified will be uploaded and other assets or modules can be included in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "timely-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create a directory for the training script\n",
    "os.makedirs('train_script', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wrapped-exhaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_script/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# FOR LOGGING TO AZURE\n",
    "from log_to_azure import LogToAzure\n",
    "from azureml.core import Run\n",
    "\n",
    "# output will be logged, separate output from previous log entries.\n",
    "print('-'*100)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data-path', type=str, \n",
    "                        dest='data_path', \n",
    "                        default='../data', \n",
    "                        help='data folder mounting point')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse the parameters passed to the this script\n",
    "    args = parse_args()\n",
    "\n",
    "    # set data paths\n",
    "    train_folder = os.path.join(args.data_path, 'train')\n",
    "    val_folder = os.path.join(args.data_path, 'validation')\n",
    "\n",
    "    print(train_folder)\n",
    "\n",
    "    # Create ImageGenerators\n",
    "    print('Creating train ImageDataGenerator')\n",
    "    train_generator = ImageDataGenerator(rescale=1/255)\\\n",
    "                            .flow_from_directory(train_folder, \n",
    "                                                 batch_size = 32)\n",
    "    val_generator = ImageDataGenerator(rescale=1/255)\\\n",
    "                            .flow_from_directory(val_folder, \n",
    "                                                 batch_size = 32)\n",
    "\n",
    "    # Build the model\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(32, (2,2), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(2,2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    metrics = [keras.metrics.CategoricalAccuracy(),\n",
    "               keras.metrics.Recall(),\n",
    "               keras.metrics.Precision()]\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=metrics)\n",
    "\n",
    "    \n",
    "    # FOR LOGGING TO AZURE\n",
    "    run = Run.get_context()\n",
    "    callbacks = [LogToAzure(run)]\n",
    "    \n",
    "    # fit model and store history\n",
    "    model.fit(train_generator, \n",
    "              validation_data=val_generator,\n",
    "              callbacks=callbacks,\n",
    "              epochs=10)\n",
    "\n",
    "    print('Done!')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01980f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../log_to_azure.py train_script/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-giant",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "together-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# setup the run details\n",
    "src = ScriptRunConfig(source_directory='train_script',\n",
    "                      script='train.py',\n",
    "                      arguments=['--data-path', dataset.as_mount()],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)\n",
    "\n",
    "# Submit the model to azure!\n",
    "run = exp.submit(config=src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd590b",
   "metadata": {},
   "source": [
    "## Create a second model\n",
    "This model is almost the same but with an added conv2d layer.  This is just so we can see different results for both models in the Azure ML Studio Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdda5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_script/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# FOR LOGGING TO AZURE\n",
    "from log_to_azure import LogToAzure\n",
    "from azureml.core import Run\n",
    "\n",
    "# output will be logged, separate output from previous log entries.\n",
    "print('-'*100)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data-path', type=str, \n",
    "                        dest='data_path', \n",
    "                        default='../data', \n",
    "                        help='data folder mounting point')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse the parameters passed to the this script\n",
    "    args = parse_args()\n",
    "\n",
    "    # set data paths\n",
    "    train_folder = os.path.join(args.data_path, 'train')\n",
    "    val_folder = os.path.join(args.data_path, 'validation')\n",
    "\n",
    "    print(train_folder)\n",
    "\n",
    "    # Create ImageGenerators\n",
    "    print('Creating train ImageDataGenerator')\n",
    "    train_generator = ImageDataGenerator(rescale=1/255)\\\n",
    "                            .flow_from_directory(train_folder, \n",
    "                                                 batch_size = 32)\n",
    "    val_generator = ImageDataGenerator(rescale=1/255)\\\n",
    "                            .flow_from_directory(val_folder, \n",
    "                                                 batch_size = 32)\n",
    "\n",
    "    # Build the model\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(32, (2,2), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(2,2))\n",
    "    model.add(keras.layers.Conv2D(16, (2,2), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(2,2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    metrics = [keras.metrics.CategoricalAccuracy(),\n",
    "               keras.metrics.Recall(),\n",
    "               keras.metrics.Precision()]\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=metrics)\n",
    "\n",
    "    \n",
    "    # FOR LOGGING TO AZURE\n",
    "    run = Run.get_context()\n",
    "    callbacks = [LogToAzure(run)]\n",
    "    \n",
    "    # fit model and store history\n",
    "    model.fit(train_generator, \n",
    "              validation_data=val_generator,\n",
    "              callbacks=callbacks,\n",
    "              epochs=10)\n",
    "\n",
    "    print('Done!')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18e4b9",
   "metadata": {},
   "source": [
    "## Run the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8128632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# setup the run details\n",
    "src = ScriptRunConfig(source_directory='train_script',\n",
    "                      script='train.py',\n",
    "                      arguments=['--data-path', dataset.as_mount()],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)\n",
    "\n",
    "# Submit the model to azure!\n",
    "run = exp.submit(config=src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
